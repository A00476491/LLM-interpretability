{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from typing import Any, Callable, Literal, TypeVar, overload\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, l1_lambda=0): # 1e-4\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.W_dec = nn.Parameter(\n",
    "            torch.nn.init.kaiming_uniform_(\n",
    "                torch.empty(\n",
    "                    hidden_dim, input_dim\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.b_dec = nn.Parameter(\n",
    "            torch.zeros(input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x - self.b_dec\n",
    "        z = self.encoder(x)\n",
    "        x_hat = z @ self.W_dec + self.b_dec\n",
    "        return x_hat, z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x_hat = z @ self.W_dec + self.b_dec\n",
    "        return x_hat\n",
    "\n",
    "    def loss(self, x, x_hat, z):\n",
    "        mse_loss = self.mse_loss_fn(x_hat, x)\n",
    "        l1_loss = torch.norm(z, 1)  # L1 regularization\n",
    "        total_loss = mse_loss + self.l1_lambda * l1_loss\n",
    "        return total_loss, mse_loss, l1_loss\n",
    "\n",
    "\n",
    "best_SAE_model = SparseAutoencoder(input_dim=896, hidden_dim=896*20).cuda()\n",
    "best_SAE_model.load_state_dict(torch.load('./model/20250328-035329/best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5828\n",
      "1803\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval().cuda()\n",
    "\n",
    "token_id_bus = str(tokenizer(\" bus\", return_tensors=\"pt\")[\"input_ids\"].numpy().tolist()[0][0])\n",
    "token_id_car = str(tokenizer(\" car\", return_tensors=\"pt\")[\"input_ids\"].numpy().tolist()[0][0])\n",
    "print(token_id_bus)\n",
    "print(token_id_car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/token_feature_count.json', 'r', encoding='utf-8') as f:\n",
    "    token_feature_count = json.load(f)\n",
    "\n",
    "# Activate features related to ' bus' and suppress other features.\n",
    "def hook_intervene_bus(module, input, output):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, z = best_SAE_model(output[0][0, -1, :])\n",
    "        z[token_feature_count[token_id_car]] = 0\n",
    "        z /= 5\n",
    "        z[token_feature_count[token_id_bus]] += 1.0\n",
    "        output[0][0, -1, :] = best_SAE_model.decode(z).reshape(-1)\n",
    "        new_output = output\n",
    "\n",
    "    return new_output\n",
    "\n",
    "# Activate features related to ' car' and suppress other features.\n",
    "def hook_intervene_car(module, input, output):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        _, z = best_SAE_model(output[0][0, -1, :])\n",
    "        z[token_feature_count[token_id_bus]] = 0\n",
    "        z /= 2\n",
    "        z[token_feature_count[token_id_car]] += 1.0\n",
    "        output[0][0, -1, :] = best_SAE_model.decode(z).reshape(-1)\n",
    "\n",
    "        new_output = output\n",
    "\n",
    "    return new_output\n",
    "\n",
    "# Use SAE to construct features during LLM inference\n",
    "def hook_SAE(module, input, output):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output[0][0, -1, :], _ = best_SAE_model(output[0][0, -1, :])\n",
    "        new_output = output\n",
    "\n",
    "    return new_output\n",
    "\n",
    "# Answer question using llm\n",
    "def generate(query = \"Generally, which is bigger, a car or a bus?\"):\n",
    "\n",
    "    prompt = f\"Give me a one-word answer\\n\\n\"\n",
    "    prompt += query\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    dummy_input = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    torch.cuda.empty_cache() \n",
    "    for i in range(20):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            out = model(dummy_input)\n",
    "            next_token_id = torch.argmax(out.logits[:, -1, :], dim=-1).unsqueeze(0)  # shape: (1, 1)\n",
    "            dummy_input = torch.cat([dummy_input, next_token_id], dim=1)\n",
    "\n",
    "        if next_token_id.item() == eos_token_id:\n",
    "            break\n",
    "        \n",
    "    if True:\n",
    "\n",
    "        full_response = tokenizer.decode(dummy_input[0], skip_special_tokens=True)\n",
    "\n",
    "        if query in full_response:\n",
    "            answer = full_response.split(query)[-1].strip().replace('assistant', '').replace('\\n', '')\n",
    "        else:\n",
    "            answer = full_response.strip().replace('assistant', '').replace('\\n', '')\n",
    "\n",
    "        print(f\"Q: {query}\")\n",
    "        print(f\"A: {answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin output:\n",
      "\n",
      "Q: Generally, which is bigger, a car or a bus?\n",
      "A: Bus.\n",
      "\n",
      "\n",
      "Through SAE:\n",
      "\n",
      "Q: Generally, which is bigger, a car or a bus?\n",
      "A:  It depends on the size and weight of the vehicle.\n",
      "\n",
      "\n",
      "Activate features of bus:\n",
      "\n",
      "Q: Generally, which is bigger, a car or a bus?\n",
      "A:  a bus. It is a more, crowded, the city of the people that the bus of the\n",
      "\n",
      "\n",
      "Activate features of car:\n",
      "\n",
      "Q: Generally, which is bigger, a car or a bus?\n",
      "A:  a car\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    handle.remove()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('Origin output:\\n')\n",
    "generate()\n",
    "\n",
    "print('\\n\\nThrough SAE:\\n')\n",
    "handle = model.model.layers[14].register_forward_hook(hook_SAE)\n",
    "generate()\n",
    "handle.remove()\n",
    "\n",
    "print('\\n\\nActivate features of bus:\\n')\n",
    "handle = model.model.layers[14].register_forward_hook(hook_intervene_bus)\n",
    "generate()\n",
    "handle.remove()\n",
    "\n",
    "print('\\n\\nActivate features of car:\\n')\n",
    "handle = model.model.layers[14].register_forward_hook(hook_intervene_car)\n",
    "generate()\n",
    "handle.remove()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
